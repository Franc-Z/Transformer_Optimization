"""
ä½¿ç”¨ NVIDIA Transformer Engine ä¼˜åŒ–çš„ Transformer å®ç°

ä¸»è¦ä¼˜åŒ–ç‚¹:
1. ä½¿ç”¨ te.Linear æ›¿æ¢ nn.Linear - é’ˆå¯¹ NVIDIA GPU é«˜åº¦ä¼˜åŒ–çš„çº¿æ€§å±‚
2. ä½¿ç”¨ te.LayerNorm æ›¿æ¢ nn.LayerNorm - èåˆçš„ LayerNorm å®ç°
3. ä½¿ç”¨ te.LayerNormLinear/LayerNormMLP è¿›ä¸€æ­¥èåˆæ“ä½œ
4. æ”¯æŒ FP16/BF16 æ··åˆç²¾åº¦è®­ç»ƒ (é€šè¿‡ torch.autocast)
5. ä¿ç•™åŸæœ‰çš„ SDPA/FlashAttention æ”¯æŒ

æ€§èƒ½å¯¹æ¯” (NVIDIA RTX A6000, seq_len=4096):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é…ç½®         â”‚ ç›¸æ¯” PyTorch åŸç”Ÿ   â”‚ ç›¸æ¯” TE-Basic    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TE-Basic     â”‚ +7-15% åŠ é€Ÿ         â”‚ åŸºå‡†            â”‚
â”‚ TE-Fused     â”‚ +20-30% åŠ é€Ÿ        â”‚ +13-17% åŠ é€Ÿ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å®‰è£… Transformer Engine:
    pip install git+https://github.com/NVIDIA/TransformerEngine.git
    
æˆ–è€…åœ¨ NGC PyTorch å®¹å™¨ (22.09+) ä¸­å·²é¢„è£…ã€‚

ä½¿ç”¨ç¤ºä¾‹:
    # æ–¹å¼1: ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºæœ€ä¼˜é…ç½®
    model = create_optimized_transformer(512, 8, 0.1)
    
    # æ–¹å¼2: ç›´æ¥ä½¿ç”¨èåˆå±‚ç‰ˆæœ¬
    model = TETransformerFused(512, 8, 0.1)
    
    # æ¨ç†æ—¶ä½¿ç”¨ autocast
    with torch.autocast(device_type='cuda', dtype=torch.float16):
        output = model(input_tensor)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.nn.attention import SDPBackend, sdpa_kernel

# å°è¯•å¯¼å…¥ Transformer Engine
try:
    import transformer_engine.pytorch as te
    TE_AVAILABLE = True
    print("âœ… Transformer Engine å·²åŠ è½½")
except ImportError:
    TE_AVAILABLE = False
    print("âš ï¸ Transformer Engine æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ PyTorch åŸç”Ÿå±‚")
    print("   å®‰è£…æ–¹æ³•: pip install git+https://github.com/NVIDIA/TransformerEngine.git")

# SDPA åç«¯æ˜ å°„ï¼ˆPyTorch 2.0+ å†…ç½® FlashAttentionï¼Œæ— éœ€é¢å¤–å®‰è£…ï¼‰
SDPA_BACKENDS = {
    "auto": None,  # è®© PyTorch è‡ªåŠ¨é€‰æ‹©
    "flash": SDPBackend.FLASH_ATTENTION,
    "efficient": SDPBackend.EFFICIENT_ATTENTION,
    "math": SDPBackend.MATH,
}


def get_available_sdpa_backends():
    """æ£€æµ‹å½“å‰ç¯å¢ƒå¯ç”¨çš„ SDPA åç«¯"""
    available = ["auto", "math"]  # math æ€»æ˜¯å¯ç”¨
    
    # æ£€æµ‹ CUDA å’Œ GPU æ¶æ„
    if torch.cuda.is_available():
        try:
            # Flash Attention éœ€è¦ sm80+ï¼ˆAmpere åŠä»¥ä¸Šï¼‰
            capability = torch.cuda.get_device_capability()
            if capability[0] >= 8:
                available.append("flash")
            available.append("efficient")  # efficient å¯¹å¤§å¤šæ•° GPU å¯ç”¨
        except:
            pass
    
    return available


print(f"å¯ç”¨çš„ SDPA åç«¯: {get_available_sdpa_backends()}")


def vanilla_attention(q, k, v, scale, dropout_p=0.0, training=False):
    """
    åŸå§‹çš„æ³¨æ„åŠ›å®ç° - æ‰‹åŠ¨è®¡ç®— softmax(QK^T / sqrt(d)) @ V
    q, k, v: [batch_size, num_heads, seq_len, head_dim]
    """
    attn_weights = torch.matmul(q, k.transpose(-2, -1)) * scale
    attn_weights = F.softmax(attn_weights, dim=-1)
    if training and dropout_p > 0:
        attn_weights = F.dropout(attn_weights, p=dropout_p)
    return torch.matmul(attn_weights, v)


def sdpa_attention(q, k, v, dropout_p=0.0, training=False, backend="auto"):
    """
    PyTorch çš„ scaled_dot_product_attentionï¼Œæ”¯æŒåç«¯é€‰æ‹©
    """
    dp = dropout_p if training else 0.0
    
    if backend == "auto" or backend not in SDPA_BACKENDS:
        return F.scaled_dot_product_attention(q, k, v, dropout_p=dp)
    else:
        with sdpa_kernel(SDPA_BACKENDS[backend]):
            return F.scaled_dot_product_attention(q, k, v, dropout_p=dp)


class PyTorchTransformer(nn.Module):
    """åŸç”Ÿ PyTorch å®ç°çš„ Transformer å— (ç”¨äºå¯¹æ¯”åŸºå‡†)"""
    
    def __init__(self, dim_hidden, mhsa_nheads, dropout, attention_type="sdpa", sdpa_backend="auto"):
        super().__init__()
        self.mhsa_nheads = mhsa_nheads
        self.dim_head = int(dim_hidden // mhsa_nheads)
        self.dim_hidden = dim_hidden
        self.dropout_p = dropout
        self.attention_type = attention_type
        self.sdpa_backend = sdpa_backend
        self.scale = 1.0 / math.sqrt(self.dim_head)
        
        # ä½¿ç”¨åŸç”Ÿ PyTorch å±‚
        self.q_proj = nn.Linear(dim_hidden, dim_hidden)
        self.k_proj = nn.Linear(dim_hidden, dim_hidden)
        self.v_proj = nn.Linear(dim_hidden, dim_hidden)
        self.linear_cat = nn.Linear(dim_hidden, dim_hidden)
        self.norm1 = nn.LayerNorm(dim_hidden)
        self.norm2 = nn.LayerNorm(dim_hidden)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.linear1 = nn.Linear(dim_hidden, dim_hidden)
        self.linear2 = nn.Linear(dim_hidden, dim_hidden)
        self.act = nn.ReLU()

    def _attn_block(self, x):
        batch_size, seq_len, _ = x.shape
        
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        q = q.view(batch_size, seq_len, self.mhsa_nheads, self.dim_head).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.mhsa_nheads, self.dim_head).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.mhsa_nheads, self.dim_head).transpose(1, 2)
        
        if self.attention_type == "vanilla":
            x = vanilla_attention(q, k, v, self.scale, self.dropout_p, self.training)
        else:
            x = sdpa_attention(q, k, v, self.dropout_p, self.training, self.sdpa_backend)
        
        x = x.transpose(1, 2).reshape(batch_size, seq_len, self.dim_hidden)
        x = self.linear_cat(x)
        return self.dropout1(x)

    def ff_block(self, x):
        x = self.linear2(self.dropout1(self.act(self.linear1(x)) ** 2))
        return self.dropout2(x)

    def forward(self, x):
        x = self.norm1(x + self._attn_block(x))
        x = self.norm2(x + self.ff_block(x))
        return x


class TETransformer(nn.Module):
    """ä½¿ç”¨ NVIDIA Transformer Engine ä¼˜åŒ–çš„ Transformer å—
    
    ä¼˜åŒ–ç‚¹:
    1. te.Linear: é’ˆå¯¹ NVIDIA GPU ä¼˜åŒ–çš„çº¿æ€§å±‚ï¼Œæ”¯æŒèåˆæ“ä½œ
    2. te.LayerNorm: ä¼˜åŒ–çš„ LayerNorm å®ç°
    3. ä¸ torch.autocast å®Œç¾é…åˆï¼Œæ”¯æŒ FP16/BF16 æ··åˆç²¾åº¦
    
    Args:
        dim_hidden: éšè—å±‚ç»´åº¦
        mhsa_nheads: æ³¨æ„åŠ›å¤´æ•°
        dropout: dropout æ¦‚ç‡
        attention_type: æ³¨æ„åŠ›å®ç°ç±»å‹ ("sdpa" æˆ– "vanilla")
        sdpa_backend: SDPA åç«¯é€‰æ‹©
    """
    
    def __init__(self, dim_hidden, mhsa_nheads, dropout, attention_type="sdpa", sdpa_backend="auto"):
        super().__init__()
        
        if not TE_AVAILABLE:
            raise ImportError("Transformer Engine æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install git+https://github.com/NVIDIA/TransformerEngine.git")
        
        self.mhsa_nheads = mhsa_nheads
        self.dim_head = int(dim_hidden // mhsa_nheads)
        self.dim_hidden = dim_hidden
        self.dropout_p = dropout
        self.attention_type = attention_type
        self.sdpa_backend = sdpa_backend
        self.scale = 1.0 / math.sqrt(self.dim_head)
        
        # ========================================
        # ä½¿ç”¨ Transformer Engine ä¼˜åŒ–å±‚
        # ========================================
        
        # QKV æŠ•å½±å±‚ - ä½¿ç”¨ TE çš„ Linear
        self.q_proj = te.Linear(dim_hidden, dim_hidden)
        self.k_proj = te.Linear(dim_hidden, dim_hidden)
        self.v_proj = te.Linear(dim_hidden, dim_hidden)
        self.linear_cat = te.Linear(dim_hidden, dim_hidden)
        
        # LayerNorm - ä½¿ç”¨ TE çš„ LayerNorm
        self.norm1 = te.LayerNorm(dim_hidden)
        self.norm2 = te.LayerNorm(dim_hidden)
        
        # FFN å±‚ - ä½¿ç”¨ TE çš„ Linear
        self.linear1 = te.Linear(dim_hidden, dim_hidden)
        self.linear2 = te.Linear(dim_hidden, dim_hidden)
        
        # Dropout ä¿æŒä½¿ç”¨ PyTorch åŸç”Ÿ
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.act = nn.ReLU()

    def _attn_block(self, x):
        batch_size, seq_len, _ = x.shape
        
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        q = q.view(batch_size, seq_len, self.mhsa_nheads, self.dim_head).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.mhsa_nheads, self.dim_head).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.mhsa_nheads, self.dim_head).transpose(1, 2)
        
        if self.attention_type == "vanilla":
            x = vanilla_attention(q, k, v, self.scale, self.dropout_p, self.training)
        else:
            x = sdpa_attention(q, k, v, self.dropout_p, self.training, self.sdpa_backend)
        
        x = x.transpose(1, 2).reshape(batch_size, seq_len, self.dim_hidden)
        x = self.linear_cat(x)
        return self.dropout1(x)

    def ff_block(self, x):
        x = self.linear2(self.dropout1(self.act(self.linear1(x)) ** 2))
        return self.dropout2(x)

    def forward(self, x):
        x = self.norm1(x + self._attn_block(x))
        x = self.norm2(x + self.ff_block(x))
        return x


class TETransformerFused(nn.Module):
    """ä½¿ç”¨ Transformer Engine èåˆå±‚çš„é«˜åº¦ä¼˜åŒ– Transformer å—
    
    è¿›ä¸€æ­¥ä¼˜åŒ–:
    1. te.LayerNormLinear: èåˆ LayerNorm + Linear æ“ä½œ
    2. te.LayerNormMLP: èåˆ LayerNorm + MLP æ“ä½œ (å¦‚å¯ç”¨)
    
    è¿™ç§èåˆå¯ä»¥å‡å°‘å†…å­˜è®¿é—®å’Œ kernel å¯åŠ¨å¼€é”€ã€‚
    """
    
    def __init__(self, dim_hidden, mhsa_nheads, dropout, attention_type="sdpa", sdpa_backend="auto"):
        super().__init__()
        
        if not TE_AVAILABLE:
            raise ImportError("Transformer Engine æœªå®‰è£…")
        
        self.mhsa_nheads = mhsa_nheads
        self.dim_head = int(dim_hidden // mhsa_nheads)
        self.dim_hidden = dim_hidden
        self.dropout_p = dropout
        self.attention_type = attention_type
        self.sdpa_backend = sdpa_backend
        self.scale = 1.0 / math.sqrt(self.dim_head)
        
        # ========================================
        # ä½¿ç”¨èåˆå±‚
        # ========================================
        
        # å°è¯•ä½¿ç”¨ LayerNormLinear (èåˆ LN + Linear)
        try:
            self.qkv_ln_linear = te.LayerNormLinear(
                dim_hidden, dim_hidden * 3,  # åŒæ—¶ç”Ÿæˆ Q, K, V
                eps=1e-5,
            )
            self.use_fused_qkv = True
        except (AttributeError, TypeError):
            # å¦‚æœä¸æ”¯æŒï¼Œå›é€€åˆ°åˆ†ç¦»çš„å±‚
            self.norm1 = te.LayerNorm(dim_hidden)
            self.q_proj = te.Linear(dim_hidden, dim_hidden)
            self.k_proj = te.Linear(dim_hidden, dim_hidden)
            self.v_proj = te.Linear(dim_hidden, dim_hidden)
            self.use_fused_qkv = False
        
        self.linear_cat = te.Linear(dim_hidden, dim_hidden)
        
        # å°è¯•ä½¿ç”¨ LayerNormMLP (èåˆ LN + MLP)
        try:
            self.ffn = te.LayerNormMLP(
                dim_hidden, dim_hidden,  # FFN hidden size ä¸ input ç›¸åŒ
                eps=1e-5,
                activation='relu',
            )
            self.use_fused_ffn = True
        except (AttributeError, TypeError):
            # å¦‚æœä¸æ”¯æŒï¼Œå›é€€åˆ°åˆ†ç¦»çš„å±‚
            self.norm2 = te.LayerNorm(dim_hidden)
            self.linear1 = te.Linear(dim_hidden, dim_hidden)
            self.linear2 = te.Linear(dim_hidden, dim_hidden)
            self.act = nn.ReLU()
            self.use_fused_ffn = False
        
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def _attn_block(self, x):
        batch_size, seq_len, _ = x.shape
        
        if self.use_fused_qkv:
            # èåˆçš„ LayerNorm + QKV æŠ•å½±
            qkv = self.qkv_ln_linear(x)
            qkv = qkv.view(batch_size, seq_len, 3, self.mhsa_nheads, self.dim_head)
            qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, batch, heads, seq, head_dim]
            q, k, v = qkv[0], qkv[1], qkv[2]
        else:
            x_norm = self.norm1(x)
            q = self.q_proj(x_norm)
            k = self.k_proj(x_norm)
            v = self.v_proj(x_norm)
            
            q = q.view(batch_size, seq_len, self.mhsa_nheads, self.dim_head).transpose(1, 2)
            k = k.view(batch_size, seq_len, self.mhsa_nheads, self.dim_head).transpose(1, 2)
            v = v.view(batch_size, seq_len, self.mhsa_nheads, self.dim_head).transpose(1, 2)
        
        if self.attention_type == "vanilla":
            x_out = vanilla_attention(q, k, v, self.scale, self.dropout_p, self.training)
        else:
            x_out = sdpa_attention(q, k, v, self.dropout_p, self.training, self.sdpa_backend)
        
        x_out = x_out.transpose(1, 2).reshape(batch_size, seq_len, self.dim_hidden)
        x_out = self.linear_cat(x_out)
        return self.dropout1(x_out)

    def ff_block(self, x):
        if self.use_fused_ffn:
            # èåˆçš„ LayerNorm + MLP
            return self.dropout2(self.ffn(x))
        else:
            x = self.linear2(self.dropout1(self.act(self.linear1(x)) ** 2))
            return self.dropout2(x)

    def forward(self, x):
        if self.use_fused_qkv:
            # æ³¨æ„ï¼šèåˆ QKV æ—¶ï¼ŒLayerNorm åœ¨ _attn_block å†…éƒ¨æ‰§è¡Œ
            x = x + self._attn_block(x)
        else:
            x = self.norm1(x + self._attn_block(x))
        
        if self.use_fused_ffn:
            x = x + self.ff_block(x)
        else:
            x = self.norm2(x + self.ff_block(x))
        
        return x


class TETransformerLayer(nn.Module):
    """ä½¿ç”¨ te.TransformerLayer çš„æœ€é«˜æ•ˆå®ç°
    
    te.TransformerLayer æ˜¯ Transformer Engine æä¾›çš„å®Œæ•´ Transformer å±‚å®ç°ï¼Œ
    åŒ…å«äº†æ‰€æœ‰å¯èƒ½çš„èåˆä¼˜åŒ–ï¼ŒåŒ…æ‹¬ï¼š
    - Fused QKV æŠ•å½±
    - FlashAttention
    - Fused LayerNorm + Linear
    - Fused LayerNorm + MLP
    - FP8 æ”¯æŒ (åœ¨ Hopper GPU ä¸Š)
    """
    
    def __init__(self, dim_hidden, mhsa_nheads, dropout, ffn_hidden_size=None):
        super().__init__()
        
        if not TE_AVAILABLE:
            raise ImportError("Transformer Engine æœªå®‰è£…")
        
        self.dim_hidden = dim_hidden
        self.mhsa_nheads = mhsa_nheads
        
        if ffn_hidden_size is None:
            ffn_hidden_size = dim_hidden  # ä¿æŒä¸åŸå§‹æ¨¡å‹ä¸€è‡´
        
        # ä½¿ç”¨ TE çš„ TransformerLayer
        self.layer = te.TransformerLayer(
            hidden_size=dim_hidden,
            ffn_hidden_size=ffn_hidden_size,
            num_attention_heads=mhsa_nheads,
            hidden_dropout=dropout,
            attention_dropout=dropout,
            self_attn_mask_type="no_mask",  # è‡ªæ³¨æ„åŠ›ï¼Œæ— å› æœæ©ç 
            layer_type="encoder",  # encoder å±‚
            fuse_qkv_params=True,  # èåˆ QKV å‚æ•°
        )
    
    def forward(self, x):
        # TransformerLayer éœ€è¦ (seq_len, batch, hidden) æ ¼å¼
        x = x.transpose(0, 1)  # [batch, seq, hidden] -> [seq, batch, hidden]
        x = self.layer(x)
        x = x.transpose(0, 1)  # [seq, batch, hidden] -> [batch, seq, hidden]
        return x


def create_optimized_transformer(
    dim_hidden, 
    mhsa_nheads, 
    dropout, 
    backend="te_fused",
    attention_type="sdpa",
    sdpa_backend="auto",
    use_compile=False,
    compile_mode="default"
):
    """åˆ›å»ºæœ€ä¼˜é…ç½®çš„ Transformer å—
    
    Args:
        dim_hidden: éšè—å±‚ç»´åº¦
        mhsa_nheads: æ³¨æ„åŠ›å¤´æ•°  
        dropout: dropout æ¦‚ç‡
        backend: åç«¯é€‰æ‹©
            - "te_fused": ä½¿ç”¨ TE èåˆå±‚ (æ¨èï¼Œæœ€å¿«)
            - "te_basic": ä½¿ç”¨ TE åŸºç¡€å±‚
            - "te_layer": ä½¿ç”¨ TE TransformerLayer
            - "pytorch": ä½¿ç”¨ PyTorch åŸç”Ÿå±‚
        attention_type: æ³¨æ„åŠ›ç±»å‹ ("sdpa" æˆ– "vanilla")
        sdpa_backend: SDPA åç«¯é€‰æ‹©
        use_compile: æ˜¯å¦ä½¿ç”¨ torch.compile è¿›è¡Œ JIT ç¼–è¯‘
        compile_mode: torch.compile çš„ç¼–è¯‘æ¨¡å¼
            - "default": å¹³è¡¡ç¼–è¯‘æ—¶é—´å’Œæ€§èƒ½
            - "reduce-overhead": å‡å°‘ Python å¼€é”€ (æ¨èç”¨äºæ¨ç†)
            - "max-autotune": æœ€å¤§åŒ–æ€§èƒ½ (ç¼–è¯‘æ—¶é—´è¾ƒé•¿)
    
    Returns:
        nn.Module: Transformer å— (å¦‚å¯ç”¨ compile åˆ™è¿”å›ç¼–è¯‘åçš„æ¨¡å‹)
    
    Example:
        >>> # åŸºç¡€ä½¿ç”¨
        >>> model = create_optimized_transformer(512, 8, 0.1).cuda()
        >>> 
        >>> # ä½¿ç”¨ torch.compile åŠ é€Ÿ
        >>> model = create_optimized_transformer(512, 8, 0.1, use_compile=True).cuda()
        >>> 
        >>> with torch.autocast(device_type='cuda', dtype=torch.float16):
        ...     output = model(input_tensor)
    """
    # åˆ›å»ºæ¨¡å‹
    if backend == "te_fused" and TE_AVAILABLE:
        model = TETransformerFused(dim_hidden, mhsa_nheads, dropout, attention_type, sdpa_backend)
    elif backend == "te_basic" and TE_AVAILABLE:
        model = TETransformer(dim_hidden, mhsa_nheads, dropout, attention_type, sdpa_backend)
    elif backend == "te_layer" and TE_AVAILABLE:
        model = TETransformerLayer(dim_hidden, mhsa_nheads, dropout)
    elif backend == "pytorch":
        model = PyTorchTransformer(dim_hidden, mhsa_nheads, dropout, attention_type, sdpa_backend)
    else:
        # é»˜è®¤å›é€€
        if TE_AVAILABLE:
            print(f"âš ï¸ æœªçŸ¥åç«¯ '{backend}'ï¼Œä½¿ç”¨ te_fused")
            model = TETransformerFused(dim_hidden, mhsa_nheads, dropout, attention_type, sdpa_backend)
        else:
            print(f"âš ï¸ Transformer Engine ä¸å¯ç”¨ï¼Œä½¿ç”¨ PyTorch åŸç”Ÿå®ç°")
            model = PyTorchTransformer(dim_hidden, mhsa_nheads, dropout, attention_type, sdpa_backend)
    
    # åº”ç”¨ torch.compile (å¦‚å¯ç”¨)
    if use_compile:
        model = compile_model(model, mode=compile_mode)
    
    return model


def compile_model(model, mode="default", dynamic=False, fullgraph=False):
    """ä½¿ç”¨ torch.compile ç¼–è¯‘æ¨¡å‹
    
    torch.compile æ˜¯ PyTorch 2.0+ çš„ JIT ç¼–è¯‘å™¨ï¼Œå¯ä»¥ï¼š
    - èåˆç®—å­ (operator fusion)
    - ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
    - ç”Ÿæˆé«˜æ•ˆçš„ CUDA ä»£ç 
    
    Args:
        model: è¦ç¼–è¯‘çš„æ¨¡å‹
        mode: ç¼–è¯‘æ¨¡å¼
            - "default": å¹³è¡¡ç¼–è¯‘æ—¶é—´å’Œè¿è¡Œæ—¶æ€§èƒ½
            - "reduce-overhead": å‡å°‘ Python å¼€é”€ï¼Œé€‚åˆæ¨ç†
            - "max-autotune": æœ€å¤§åŒ–æ€§èƒ½ï¼Œç¼–è¯‘æ—¶é—´é•¿ï¼Œé€‚åˆé‡å¤è¿è¡Œçš„æ¨¡å‹
        dynamic: æ˜¯å¦æ”¯æŒåŠ¨æ€å½¢çŠ¶ (å¦‚ batch_size/seq_len å˜åŒ–)
        fullgraph: æ˜¯å¦è¦æ±‚ç¼–è¯‘æ•´ä¸ªè®¡ç®—å›¾ (å¤±è´¥æ—¶æŠ¥é”™è€Œéå›é€€)
    
    Returns:
        ç¼–è¯‘åçš„æ¨¡å‹
        
    Example:
        >>> model = TETransformerFused(512, 8, 0.1).cuda()
        >>> compiled_model = compile_model(model, mode="reduce-overhead")
        >>> # é¦–æ¬¡è°ƒç”¨ä¼šè§¦å‘ç¼–è¯‘ (è¾ƒæ…¢)ï¼Œåç»­è°ƒç”¨ä¼šå¾ˆå¿«
        >>> output = compiled_model(input_tensor)
    
    æ³¨æ„:
        1. é¦–æ¬¡è°ƒç”¨ç¼–è¯‘åçš„æ¨¡å‹ä¼šè§¦å‘ç¼–è¯‘ï¼Œè€—æ—¶è¾ƒé•¿
        2. ç¼–è¯‘åçš„æ¨¡å‹å¯¹äºç›¸åŒå½¢çŠ¶çš„è¾“å…¥ä¼šæ›´å¿«
        3. å¦‚æœè¾“å…¥å½¢çŠ¶å˜åŒ–ï¼Œå¯èƒ½éœ€è¦é‡æ–°ç¼–è¯‘ (é™¤é dynamic=True)
        4. ä¸ CUDA Graph ä¸å…¼å®¹ï¼ŒäºŒé€‰ä¸€ä½¿ç”¨
        
    æ¨èä½¿ç”¨ warmup_compiled_model() è¿›è¡Œé¢„çƒ­ä»¥é¿å…è®­ç»ƒæ—¶é¦–ä¸ª step è¿‡æ…¢
    """
    try:
        compiled = torch.compile(
            model,
            mode=mode,
            dynamic=dynamic,
            fullgraph=fullgraph,
        )
        return compiled
    except Exception as e:
        print(f"âš ï¸ torch.compile å¤±è´¥: {e}")
        print("   å›é€€åˆ°æœªç¼–è¯‘æ¨¡å‹")
        return model


def warmup_compiled_model(
    model, 
    sample_input, 
    dtype=torch.float16, 
    warmup_steps=3,
    verbose=True
):
    """é¢„çƒ­ç¼–è¯‘åçš„æ¨¡å‹ï¼Œè§¦å‘ JIT ç¼–è¯‘
    
    torch.compile çš„ç¼–è¯‘æ˜¯æ‡’åŠ è½½çš„ï¼ˆlazyï¼‰ï¼Œåªæœ‰åœ¨é¦–æ¬¡è°ƒç”¨æ—¶æ‰ä¼šçœŸæ­£ç¼–è¯‘ã€‚
    è¿™ä¼šå¯¼è‡´è®­ç»ƒæ—¶ç¬¬ä¸€ä¸ª step ç‰¹åˆ«æ…¢ï¼ˆå¯èƒ½å‡ åç§’åˆ°å‡ åˆ†é’Ÿï¼‰ã€‚
    
    æ­¤å‡½æ•°åœ¨è®­ç»ƒå¼€å§‹å‰é¢„çƒ­æ¨¡å‹ï¼Œæå‰è§¦å‘ç¼–è¯‘è¿‡ç¨‹ã€‚
    
    Args:
        model: ç¼–è¯‘åçš„æ¨¡å‹ (torch.compile è¿”å›çš„å¯¹è±¡)
        sample_input: ä¸è®­ç»ƒæ•°æ®å½¢çŠ¶ç›¸åŒçš„æ ·ä¾‹è¾“å…¥
        dtype: æ•°æ®ç²¾åº¦ (ä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„ç²¾åº¦ä¸€è‡´)
        warmup_steps: é¢„çƒ­æ­¥æ•° (é€šå¸¸ 1-3 æ­¥å³å¯è§¦å‘å®Œæ•´ç¼–è¯‘)
        verbose: æ˜¯å¦æ‰“å°é¢„çƒ­ä¿¡æ¯
    
    Returns:
        é¢„çƒ­åçš„æ¨¡å‹ (åŒä¸€å¯¹è±¡)
        
    Example:
        >>> # åˆ›å»ºå¹¶ç¼–è¯‘æ¨¡å‹
        >>> model = create_optimized_transformer(512, 8, 0.1, use_compile=True).cuda()
        >>> 
        >>> # åˆ›å»ºæ ·ä¾‹è¾“å…¥ (ä¸è®­ç»ƒæ•°æ®å½¢çŠ¶ç›¸åŒ)
        >>> sample = torch.randn(batch_size, seq_len, dim_hidden, device='cuda')
        >>> 
        >>> # é¢„çƒ­ (è§¦å‘ç¼–è¯‘ï¼Œè€—æ—¶è¾ƒé•¿ä½†åªéœ€ä¸€æ¬¡)
        >>> model = warmup_compiled_model(model, sample)
        >>> 
        >>> # ç°åœ¨å¼€å§‹è®­ç»ƒï¼Œç¬¬ä¸€ä¸ª step ä¸ä¼šç‰¹åˆ«æ…¢äº†
        >>> for batch in dataloader:
        ...     output = model(batch)
    
    è®­ç»ƒæœ€ä½³å®è·µ:
        1. è°ƒè¯•é˜¶æ®µï¼šä¸ä½¿ç”¨ torch.compileï¼Œæ–¹ä¾¿æ’é”™
        2. æ­£å¼è®­ç»ƒï¼šå¼€å¯ torch.compile + è®­ç»ƒå‰è°ƒç”¨ warmup_compiled_model()
        3. ä¿æŒ batch_size å›ºå®šï¼Œé¿å…è§¦å‘é‡å¤ç¼–è¯‘
    """
    import time
    
    if verbose:
        print("ğŸ”¥ å¼€å§‹é¢„çƒ­ç¼–è¯‘æ¨¡å‹...")
        print(f"   è¾“å…¥å½¢çŠ¶: {sample_input.shape}")
        print(f"   é¢„çƒ­æ­¥æ•°: {warmup_steps}")
        start_time = time.time()
    
    model.train()  # ç¡®ä¿åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ç¼–è¯‘ï¼ˆä¼šåŒæ—¶ç¼–è¯‘ forward å’Œ backwardï¼‰
    
    for step in range(warmup_steps):
        # å‰å‘ä¼ æ’­
        with torch.autocast(device_type='cuda', dtype=dtype):
            output = model(sample_input)
            # æ¨¡æ‹ŸæŸå¤±è®¡ç®—
            loss = output.mean()
        
        # åå‘ä¼ æ’­ (ä¹Ÿéœ€è¦ç¼–è¯‘)
        loss.backward()
        
        # æ¸…ç†æ¢¯åº¦
        model.zero_grad(set_to_none=True)
        
        if verbose:
            print(f"   Step {step + 1}/{warmup_steps} å®Œæˆ")
    
    # åŒæ­¥ CUDA
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    if verbose:
        elapsed = time.time() - start_time
        print(f"âœ… é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
        print("   åç»­è®­ç»ƒ step å°†ä»¥æ­£å¸¸é€Ÿåº¦è¿è¡Œ")
    
    return model


def create_compiled_model_for_training(
    dim_hidden,
    mhsa_nheads,
    dropout,
    backend="te_fused",
    sample_input=None,
    dtype=torch.float16,
    compile_mode="default",
    warmup=True,
    verbose=True
):
    """åˆ›å»ºå¹¶é¢„çƒ­ç”¨äºè®­ç»ƒçš„ç¼–è¯‘æ¨¡å‹ï¼ˆä¸€ç«™å¼å‡½æ•°ï¼‰
    
    è¿™ä¸ªå‡½æ•°æ•´åˆäº†æ¨¡å‹åˆ›å»ºã€ç¼–è¯‘å’Œé¢„çƒ­çš„å®Œæ•´æµç¨‹ã€‚
    
    Args:
        dim_hidden: éšè—å±‚ç»´åº¦
        mhsa_nheads: æ³¨æ„åŠ›å¤´æ•°
        dropout: dropout æ¦‚ç‡
        backend: åç«¯é€‰æ‹© ("te_fused", "te_basic", "pytorch")
        sample_input: æ ·ä¾‹è¾“å…¥å¼ é‡ (ç”¨äºé¢„çƒ­ï¼Œéœ€è¦ä¸è®­ç»ƒæ•°æ®å½¢çŠ¶ä¸€è‡´)
        dtype: æ•°æ®ç²¾åº¦
        compile_mode: ç¼–è¯‘æ¨¡å¼ ("default" æ¨èç”¨äºè®­ç»ƒ)
        warmup: æ˜¯å¦è¿›è¡Œé¢„çƒ­
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        é¢„çƒ­å®Œæˆçš„ç¼–è¯‘æ¨¡å‹ï¼Œå¯ç›´æ¥ç”¨äºè®­ç»ƒ
        
    Example:
        >>> # ä¸€ç«™å¼åˆ›å»ºç”¨äºè®­ç»ƒçš„æ¨¡å‹
        >>> sample = torch.randn(32, 4096, 512, device='cuda')
        >>> model = create_compiled_model_for_training(
        ...     dim_hidden=512,
        ...     mhsa_nheads=8,
        ...     dropout=0.1,
        ...     sample_input=sample,
        ...     compile_mode="default"
        ... )
        >>> 
        >>> # ç›´æ¥å¼€å§‹è®­ç»ƒ
        >>> optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        >>> for batch in dataloader:
        ...     optimizer.zero_grad()
        ...     with torch.autocast(device_type='cuda', dtype=torch.float16):
        ...         output = model(batch)
        ...         loss = loss_fn(output, target)
        ...     loss.backward()
        ...     optimizer.step()
    """
    if verbose:
        print("=" * 60)
        print("  åˆ›å»ºç¼–è¯‘è®­ç»ƒæ¨¡å‹")
        print("=" * 60)
    
    # 1. åˆ›å»ºæ¨¡å‹
    model = create_optimized_transformer(
        dim_hidden, mhsa_nheads, dropout,
        backend=backend,
        use_compile=False  # å…ˆä¸ç¼–è¯‘ï¼Œåé¢å•ç‹¬å¤„ç†
    )
    
    if torch.cuda.is_available():
        model = model.cuda()
    
    if verbose:
        print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: {type(model).__name__}")
    
    # 2. ç¼–è¯‘æ¨¡å‹
    model = compile_model(model, mode=compile_mode)
    if verbose:
        print(f"âœ… æ¨¡å‹ç¼–è¯‘å®Œæˆ (mode={compile_mode})")
    
    # 3. é¢„çƒ­ (å¦‚æœæä¾›äº†æ ·ä¾‹è¾“å…¥)
    if warmup and sample_input is not None:
        model = warmup_compiled_model(model, sample_input, dtype=dtype, verbose=verbose)
    elif warmup and sample_input is None:
        if verbose:
            print("âš ï¸ æœªæä¾› sample_inputï¼Œè·³è¿‡é¢„çƒ­")
            print("   è®­ç»ƒæ—¶ç¬¬ä¸€ä¸ª step ä¼šè¾ƒæ…¢ (è§¦å‘ç¼–è¯‘)")
    
    return model


class CUDAGraphWrapper:
    """CUDA Graph åŒ…è£…å™¨"""
    def __init__(self, model, sample_input, warmup_iters=3):
        self.model = model
        self.model.eval()
        self.static_input = sample_input.clone()
        
        with torch.no_grad():
            for _ in range(warmup_iters):
                _ = self.model(self.static_input)
        
        torch.cuda.synchronize()
        self.graph = torch.cuda.CUDAGraph()
        
        with torch.cuda.graph(self.graph):
            self.static_output = self.model(self.static_input)
    
    def __call__(self, x):
        self.static_input.copy_(x)
        self.graph.replay()
        return self.static_output.clone()


def benchmark(func, warmup=10, iterations=100):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œè¿”å›å¹³å‡è€—æ—¶(ms)"""
    for _ in range(warmup):
        func()
    
    torch.cuda.synchronize()
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    
    start.record()
    for _ in range(iterations):
        func()
    end.record()
    
    torch.cuda.synchronize()
    return start.elapsed_time(end) / iterations


def benchmark_with_autocast(model, inp, dtype=torch.float16, warmup=10, iterations=100):
    """ä½¿ç”¨ autocast çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    def run():
        with torch.autocast(device_type='cuda', dtype=dtype):
            return model(inp)
    
    return benchmark(run, warmup, iterations)


if __name__ == "__main__":
    print("=" * 80)
    print("  Transformer Engine ä¼˜åŒ–æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    if not TE_AVAILABLE:
        print("\nâŒ Transformer Engine æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”æµ‹è¯•")
        print("   è¯·å…ˆå®‰è£…: pip install git+https://github.com/NVIDIA/TransformerEngine.git")
        exit(1)
    
    # æ‰“å° GPU ä¿¡æ¯
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        capability = torch.cuda.get_device_capability()
        print(f"\nğŸ–¥ï¸  GPU: {gpu_name}")
        print(f"   Compute Capability: sm{capability[0]}{capability[1]}")
    
    # é…ç½®å‚æ•°
    dim_hidden = 512
    num_heads = 8
    head_dim = dim_hidden // num_heads
    dropout = 0.1
    seq_len = 4096
    batch_sizes = [128, 64, 32, 16, 8, 4, 2, 1]
    
    # æµ‹è¯•ç²¾åº¦
    test_dtypes = [
        ("FP16", torch.float16),
        ("BF16", torch.bfloat16),
    ]
    
    # ========================================
    # Part 1: PyTorch vs Transformer Engine å¯¹æ¯”
    # ========================================
    print("\n" + "=" * 80)
    print("  Part 1: PyTorch åŸç”Ÿ vs Transformer Engine ç«¯åˆ°ç«¯æ€§èƒ½å¯¹æ¯”")
    print(f"  åºåˆ—é•¿åº¦: {seq_len}, éšè—ç»´åº¦: {dim_hidden}, æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    print("=" * 80)
    
    for dtype_name, dtype in test_dtypes:
        print(f"\nğŸ“Š ä½¿ç”¨ {dtype_name} ç²¾åº¦æµ‹è¯•:")
        print(f"{'Batch Size':<12} {'PyTorch (ms)':<15} {'TE (ms)':<15} {'åŠ é€Ÿæ¯”':<12}")
        print("-" * 55)
        
        for bs in batch_sizes:
            try:
                # åˆ›å»ºè¾“å…¥ (FP32ï¼Œautocast ä¼šå¤„ç†è½¬æ¢)
                inp = torch.randn(bs, seq_len, dim_hidden, device='cuda')
                
                # PyTorch åŸç”Ÿæ¨¡å‹
                model_pytorch = PyTorchTransformer(
                    dim_hidden, num_heads, dropout, 
                    attention_type="sdpa", sdpa_backend="auto"
                ).cuda().eval()
                
                # Transformer Engine æ¨¡å‹
                model_te = TETransformer(
                    dim_hidden, num_heads, dropout,
                    attention_type="sdpa", sdpa_backend="auto"
                ).cuda().eval()
                
                with torch.no_grad():
                    # PyTorch åŸç”Ÿ + autocast
                    try:
                        time_pytorch = benchmark_with_autocast(
                            model_pytorch, inp, dtype=dtype, warmup=5, iterations=30
                        )
                    except torch.cuda.OutOfMemoryError:
                        time_pytorch = float('inf')
                        torch.cuda.empty_cache()
                    
                    # Transformer Engine + autocast
                    try:
                        time_te = benchmark_with_autocast(
                            model_te, inp, dtype=dtype, warmup=5, iterations=30
                        )
                    except torch.cuda.OutOfMemoryError:
                        time_te = float('inf')
                        torch.cuda.empty_cache()
                
                # è®¡ç®—åŠ é€Ÿæ¯”
                if time_pytorch != float('inf') and time_te != float('inf'):
                    speedup = time_pytorch / time_te
                    pytorch_str = f"{time_pytorch:.3f}"
                    te_str = f"{time_te:.3f}"
                    speedup_str = f"{speedup:.2f}x"
                else:
                    pytorch_str = "OOM" if time_pytorch == float('inf') else f"{time_pytorch:.3f}"
                    te_str = "OOM" if time_te == float('inf') else f"{time_te:.3f}"
                    speedup_str = "-"
                
                print(f"{bs:<12} {pytorch_str:<15} {te_str:<15} {speedup_str:<12}")
                
                del model_pytorch, model_te, inp
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"{bs:<12} Error: {e}")
                torch.cuda.empty_cache()
    
    # ========================================
    # Part 2: ä¸åŒ TE é…ç½®å¯¹æ¯”
    # ========================================
    print("\n" + "=" * 80)
    print("  Part 2: TE ä¸åŒé…ç½®æ€§èƒ½å¯¹æ¯” (FP16)")
    print("=" * 80)
    
    print(f"\n{'Batch Size':<12} {'TE-Basic (ms)':<15} {'TE-Fused (ms)':<16} {'TE-Layer (ms)':<15} {'æœ€ä½³åŠ é€Ÿæ¯”':<12}")
    print("-" * 75)
    
    dtype = torch.float16
    
    for bs in batch_sizes[:5]:  # åªæµ‹è¯•éƒ¨åˆ† batch size
        try:
            inp = torch.randn(bs, seq_len, dim_hidden, device='cuda')
            
            # TE åŸºç¡€ç‰ˆæœ¬
            model_te = TETransformer(
                dim_hidden, num_heads, dropout,
                attention_type="sdpa", sdpa_backend="auto"
            ).cuda().eval()
            
            # TE èåˆç‰ˆæœ¬ (å¦‚å¯ç”¨)
            try:
                model_te_fused = TETransformerFused(
                    dim_hidden, num_heads, dropout,
                    attention_type="sdpa", sdpa_backend="auto"
                ).cuda().eval()
                has_fused = True
            except Exception:
                has_fused = False
            
            # TE TransformerLayer (æœ€é«˜æ•ˆ)
            try:
                model_te_layer = TETransformerLayer(
                    dim_hidden, num_heads, dropout
                ).cuda().eval()
                has_layer = True
            except Exception:
                has_layer = False
            
            with torch.no_grad():
                time_te = benchmark_with_autocast(
                    model_te, inp, dtype=dtype, warmup=5, iterations=30
                )
                
                if has_fused:
                    try:
                        time_te_fused = benchmark_with_autocast(
                            model_te_fused, inp, dtype=dtype, warmup=5, iterations=30
                        )
                    except Exception:
                        time_te_fused = float('nan')
                else:
                    time_te_fused = float('nan')
                
                if has_layer:
                    try:
                        time_te_layer = benchmark_with_autocast(
                            model_te_layer, inp, dtype=dtype, warmup=5, iterations=30
                        )
                    except Exception:
                        time_te_layer = float('nan')
                else:
                    time_te_layer = float('nan')
            
            # æ‰¾æœ€ä½³æ—¶é—´
            times = [time_te]
            if time_te_fused == time_te_fused:
                times.append(time_te_fused)
            if time_te_layer == time_te_layer:
                times.append(time_te_layer)
            best_time = min(times)
            speedup = time_te / best_time
            
            fused_str = f"{time_te_fused:.3f}" if time_te_fused == time_te_fused else "N/A"
            layer_str = f"{time_te_layer:.3f}" if time_te_layer == time_te_layer else "N/A"
            
            print(f"{bs:<12} {time_te:<15.3f} {fused_str:<16} {layer_str:<15} {speedup:<12.2f}x")
            
            del model_te, inp
            if has_fused:
                del model_te_fused
            if has_layer:
                del model_te_layer
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"{bs:<12} Error: {e}")
            torch.cuda.empty_cache()
    
    # ========================================
    # Part 3: torch.compile åŠ é€Ÿæµ‹è¯•
    # ========================================
    print("\n" + "=" * 80)
    print("  Part 3: torch.compile JIT ç¼–è¯‘åŠ é€Ÿæ•ˆæœ")
    print("=" * 80)
    
    print("\nğŸ’¡ torch.compile è¯´æ˜:")
    print("   - é¦–æ¬¡è°ƒç”¨ä¼šè§¦å‘ç¼–è¯‘ (è¾ƒæ…¢)ï¼Œåç»­è°ƒç”¨åŠ é€Ÿ")
    print("   - 'reduce-overhead' æ¨¡å¼é€‚åˆæ¨ç†ï¼Œå‡å°‘ Python å¼€é”€")
    print("   - 'max-autotune' æ¨¡å¼æœ€å¿«ï¼Œä½†ç¼–è¯‘æ—¶é—´é•¿")
    
    compile_modes = ["default", "reduce-overhead", "max-autotune"]
    
    print(f"\n{'Batch Size':<12} {'TE (ms)':<12} {'default':<12} {'reduce-oh':<12} {'max-auto':<12} {'æœ€ä½³åŠ é€Ÿ':<10}")
    print("-" * 75)
    
    for bs in batch_sizes[:5]:  # åªæµ‹è¯•éƒ¨åˆ† batch size (ç¼–è¯‘è€—æ—¶)
        try:
            inp = torch.randn(bs, seq_len, dim_hidden, device='cuda')
            
            # TE åŸºç¡€æ¨¡å‹ (æœªç¼–è¯‘)
            model_te = TETransformerFused(
                dim_hidden, num_heads, dropout,
                attention_type="sdpa", sdpa_backend="auto"
            ).cuda().eval()
            
            with torch.no_grad():
                time_te = benchmark_with_autocast(model_te, inp, dtype=dtype, warmup=5, iterations=30)
            
            compile_times = {}
            for mode in compile_modes:
                try:
                    # åˆ›å»ºæ–°æ¨¡å‹å¹¶ç¼–è¯‘
                    model_compiled = TETransformerFused(
                        dim_hidden, num_heads, dropout,
                        attention_type="sdpa", sdpa_backend="auto"
                    ).cuda().eval()
                    model_compiled.load_state_dict(model_te.state_dict())
                    
                    # ç¼–è¯‘æ¨¡å‹
                    model_compiled = compile_model(model_compiled, mode=mode)
                    
                    # é¢„çƒ­ (é¦–æ¬¡è°ƒç”¨è§¦å‘ç¼–è¯‘)
                    with torch.no_grad():
                        with torch.autocast(device_type='cuda', dtype=dtype):
                            for _ in range(3):
                                _ = model_compiled(inp)
                    torch.cuda.synchronize()
                    
                    # æµ‹è¯•
                    with torch.no_grad():
                        time_compiled = benchmark_with_autocast(
                            model_compiled, inp, dtype=dtype, warmup=5, iterations=30
                        )
                    compile_times[mode] = time_compiled
                    
                    del model_compiled
                    torch.cuda.empty_cache()
                except Exception as e:
                    compile_times[mode] = float('nan')
                    torch.cuda.empty_cache()
            
            # æ‰¾æœ€ä½³ç¼–è¯‘æ—¶é—´
            valid_times = [t for t in compile_times.values() if t == t]  # æ’é™¤ NaN
            if valid_times:
                best_compile = min(valid_times)
                best_speedup = time_te / best_compile
            else:
                best_speedup = float('nan')
            
            default_str = f"{compile_times['default']:.3f}" if compile_times['default'] == compile_times['default'] else "N/A"
            reduce_str = f"{compile_times['reduce-overhead']:.3f}" if compile_times['reduce-overhead'] == compile_times['reduce-overhead'] else "N/A"
            max_str = f"{compile_times['max-autotune']:.3f}" if compile_times['max-autotune'] == compile_times['max-autotune'] else "N/A"
            speedup_str = f"{best_speedup:.2f}x" if best_speedup == best_speedup else "-"
            
            print(f"{bs:<12} {time_te:<12.3f} {default_str:<12} {reduce_str:<12} {max_str:<12} {speedup_str:<10}")
            
            del model_te, inp
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"{bs:<12} Error: {e}")
            torch.cuda.empty_cache()
    
    # ========================================
    # Part 4: CUDA Graph + TE ç»„åˆæµ‹è¯•
    # ========================================
    print("\n" + "=" * 80)
    print("  Part 4: CUDA Graph + Transformer Engine ç»„åˆåŠ é€Ÿ")
    print("=" * 80)
    
    print("\nâš ï¸ æ³¨æ„: CUDA Graph å’Œ torch.compile ä¸å…¼å®¹ï¼ŒäºŒé€‰ä¸€ä½¿ç”¨")
    
    print(f"\n{'Batch Size':<12} {'TE (ms)':<15} {'TE+Graph (ms)':<16} {'GraphåŠ é€Ÿæ¯”':<12}")
    print("-" * 60)
    
    for bs in batch_sizes:
        try:
            inp = torch.randn(bs, seq_len, dim_hidden, dtype=torch.float16, device='cuda')
            
            model_te = TETransformer(
                dim_hidden, num_heads, dropout,
                attention_type="sdpa", sdpa_backend="auto"
            ).cuda().to(torch.float16).eval()
            
            with torch.no_grad():
                # ç›´æ¥ FP16 æ¨ç† (æ—  autocast)
                time_te = benchmark(lambda: model_te(inp), warmup=5, iterations=30)
                
                try:
                    graph_model = CUDAGraphWrapper(model_te, inp, warmup_iters=5)
                    time_graph = benchmark(lambda: graph_model(inp), warmup=5, iterations=30)
                    speedup = time_te / time_graph
                    print(f"{bs:<12} {time_te:<15.3f} {time_graph:<16.3f} {speedup:<12.2f}x")
                    del graph_model
                except Exception as e:
                    print(f"{bs:<12} {time_te:<15.3f} {'Graphå¤±è´¥':<16} -")
            
            del model_te, inp
            torch.cuda.empty_cache()
            
        except torch.cuda.OutOfMemoryError:
            print(f"{bs:<12} OOM")
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"{bs:<12} Error: {e}")
            torch.cuda.empty_cache()
    
    # ========================================
    # Part 5: æ˜¾å­˜ä½¿ç”¨å¯¹æ¯”
    # ========================================
    print("\n" + "=" * 80)
    print("  Part 5: æ˜¾å­˜ä½¿ç”¨å¯¹æ¯” (FP16)")
    print("=" * 80)
    
    test_bs = 32
    print(f"\næµ‹è¯•é…ç½®: Batch Size = {test_bs}, Seq Len = {seq_len}")
    
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    # PyTorch åŸç”Ÿ
    inp = torch.randn(test_bs, seq_len, dim_hidden, device='cuda')
    model_pytorch = PyTorchTransformer(
        dim_hidden, num_heads, dropout
    ).cuda().eval()
    
    with torch.no_grad():
        with torch.autocast(device_type='cuda', dtype=torch.float16):
            _ = model_pytorch(inp)
    
    pytorch_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB
    
    del model_pytorch, inp
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    # Transformer Engine
    inp = torch.randn(test_bs, seq_len, dim_hidden, device='cuda')
    model_te = TETransformer(
        dim_hidden, num_heads, dropout
    ).cuda().eval()
    
    with torch.no_grad():
        with torch.autocast(device_type='cuda', dtype=torch.float16):
            _ = model_te(inp)
    
    te_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB
    
    del model_te, inp
    torch.cuda.empty_cache()
    
    print(f"\n{'æ¨¡å‹ç±»å‹':<20} {'å³°å€¼æ˜¾å­˜ (MB)':<20}")
    print("-" * 40)
    print(f"{'PyTorch åŸç”Ÿ':<20} {pytorch_mem:<20.1f}")
    print(f"{'Transformer Engine':<20} {te_mem:<20.1f}")
    print(f"{'æ˜¾å­˜èŠ‚çœ':<20} {(1 - te_mem/pytorch_mem)*100:<20.1f}%")
    
    # ========================================
    # æ€»ç»“
    # ========================================
    print("\n" + "=" * 80)
    print("  æ€»ç»“")
    print("=" * 80)
    
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("   1. ä½¿ç”¨ TETransformerFused å¯è·å¾— 20-30% åŠ é€Ÿ (èåˆå±‚æœ€ä¼˜)")
    print("   2. é…åˆ torch.autocast ä½¿ç”¨ FP16/BF16 æ··åˆç²¾åº¦")
    print("   3. torch.compile å¯¹ TE ä¼˜åŒ–åçš„æ¨¡å‹æ•ˆæœæœ‰é™:")
    print("      - TE å±‚å·²æ˜¯é«˜åº¦ä¼˜åŒ–çš„ CUDA å†…æ ¸ï¼Œcompile éš¾ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–")
    print("      - å¯¹ PyTorch åŸç”Ÿæ¨¡å‹å¯èƒ½æœ‰ 5-15% é¢å¤–åŠ é€Ÿ")
    print("   4. CUDA Graph ä¸ torch.compile ä¸å…¼å®¹ï¼ŒäºŒé€‰ä¸€ä½¿ç”¨")
    print("   5. å¦‚ç¡¬ä»¶æ”¯æŒï¼Œå¯è¿›ä¸€æ­¥å°è¯• FP8 ç²¾åº¦ (éœ€è¦ Hopper/Ada GPU)")
    
    print("\nğŸ“š å‚è€ƒèµ„æ–™:")
    print("   - Transformer Engine: https://github.com/NVIDIA/TransformerEngine")
    print("   - å®˜æ–¹æ–‡æ¡£: https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/")

